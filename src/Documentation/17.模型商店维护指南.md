# 模型商店维护指南

## 架构概览

模型商店通过 `modelStoreConfig.json` 配置模型列表，下载逻辑自动在三个源之间 Fallback：

```
用户点击下载 → HuggingFace(全球首选) → HF-Mirror(中国Fallback) → ModelScope(中国备用)
```

| 源 | 域名 | URL 模板 | 特点 |
|---|------|---------|------|
| HF-Mirror | hf-mirror.com | `/{repo}/resolve/main/{fileName}` | 国内 HuggingFace 镜像 |
| ModelScope | modelscope.cn | `/models/{repo}/resolve/master/{fileName}` | 魔搭社区，注意分支是 `master` |
| HuggingFace | huggingface.co | `/{repo}/resolve/main/{fileName}` | 全球原站 |

## 配置文件说明

路径：`electron/local/modelStoreConfig.json`

```json
{
    "id": "qwen3-8b",           // 唯一 ID
    "name": "Qwen3 8B",         // 显示名称
    "icon": "qwen",             // 图标标识
    "description": "...",       // 模型描述
    "huggingfaceRepo": "Qwen/Qwen3-8B-GGUF",  // HuggingFace 仓库路径（必填）
    "modelScopeRepo": "Qwen/Qwen3-8B-GGUF",   // ModelScope 仓库路径（可选）
    "variants": [
        {
            "quantization": "Q4_K_M",          // 量化级别
            "fileName": "Qwen3-8B-Q4_K_M.gguf",// 精确文件名（大小写敏感！）
            "fileSize": 5027783488,             // 文件大小（字节）
            "recommendedVRAM": "6 GB"           // 推荐的 VRAM
        }
    ]
}
```

## 添加新模型步骤

### 1. 验证 HuggingFace 仓库

```bash
# 查看仓库文件列表和精确文件名
curl https://huggingface.co/api/models/{owner}/{repo}/tree/main

# 示例
curl https://huggingface.co/api/models/Qwen/Qwen3-8B-GGUF/tree/main
```

> ⚠️ 如果返回 401，说明仓库需认证或不存在，**不能使用**。

### 2. 验证 ModelScope 仓库（可选但推荐）

```bash
curl https://modelscope.cn/api/v1/models/{owner}/{repo}/repo/files
```

> 注意：ModelScope 部分模型的 GGUF 文件可能被分到子目录中（分片模型），这种**不能使用**，只支持单文件 GGUF。

### 3. 确认文件名和大小

从 API 返回的 JSON 中提取：

```json
{
    "path": "Qwen3-8B-Q4_K_M.gguf",     ← fileName（精确大小写！）
    "size": 5027783488                    ← fileSize
}
```

> ⚠️ **fileName 必须与 HuggingFace 上完全一致，包括大小写！** 这是最常见的错误。

### 4. 添加配置

在 `modelStoreConfig.json` 数组中添加新条目，建议每个模型提供 3 个量化级别：
- `Q4_K_M` — 最小/最快，适合低配 GPU
- `Q6_K` — 中间级别
- `Q8_0` — 最大/最好质量

### 5. 测试下载

重启应用后在模型商店页面测试下载功能。观察日志中的下载源切换：

```
[ModelStore] 尝试源: HF-Mirror → https://hf-mirror.com/...
[ModelStore] [HF-Mirror] 连接成功，开始传输
```

## 下载逻辑说明

代码路径：`electron/ipc/localModelHandlers.ts` → `model:download-model`

- **Fallback 机制**：按 HF-Mirror → ModelScope → HuggingFace 顺序尝试
- **超时**：每个源 15 秒连接超时，超时后自动切换下一源
- **断点续传**：支持 HTTP Range 请求，下载中断后可续传（检查 `.downloading` 临时文件）
- **重定向**：最多 5 次重定向跟踪
- **取消**：用户可随时取消下载

## 当前模型列表

| 模型 | 仓库 | 参数量 | VRAM 范围 | 状态 |
|------|------|--------|-----------|------|
| Qwen3 8B | Qwen/Qwen3-8B-GGUF | 8B | 6-10 GB | ✅ |
| Qwen3 32B | unsloth/Qwen3-32B-GGUF | 32B | 24-48 GB | ✅ |
| DeepSeek R1 0528 8B | unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF | 8B | 6-10 GB | ✅ |
| Llama 3.1 8B | bartowski/Meta-Llama-3.1-8B-Instruct-GGUF | 8B | 6-10 GB | ✅ |
| Phi 4 Mini | bartowski/microsoft_Phi-4-mini-instruct-GGUF | 3.8B | 4-6 GB | ✅ |
| Gemma 3 4B | bartowski/google_gemma-3-4b-it-GGUF | 4B | 4-6 GB | ✅ |
| GLM-4 9B | bartowski/glm-4-9b-chat-GGUF | 9B | 8-12 GB | ✅ |
| Yi 1.5 9B | bartowski/Yi-1.5-9B-Chat-GGUF | 9B | 6-12 GB | ✅ |
| InternLM3 8B | bartowski/internlm3-8b-instruct-GGUF | 8B | 6-10 GB | ✅ |

## 模型图标资源

图标库推荐使用 **LobeHub Icons**：

- 官网浏览：https://lobehub.com/zh/icons
- GitHub：https://github.com/lobehub/lobe-icons
- npm 包：`@lobehub/icons`

### 当前模型对应的 LobeHub 图标名

| 模型 | 配置中 icon | LobeHub 组件名 | 备注 |
|------|------------|---------------|------|
| Qwen3 8B / 32B | `qwen` | `<Qwen />` | ✅ 有对应图标 |
| DeepSeek R1 | `deepseek` | `<DeepSeek />` | ✅ 有对应图标 |
| Llama 3.1 | `llama` | `<Meta />` | ✅ 搜索 "Meta" 或 "Llama" |
| Phi 4 Mini | `phi` | 无 | ❌ LobeHub 暂无 Phi 图标，需自行准备 |
| Gemma 3 | `gemma` | `<Gemma />` | ✅ 有对应图标 |
| GLM-4 | `glm` | `<ChatGLM />` 或 `<GLMV />` | ✅ 有对应图标，注意组件名不同 |
| Yi 1.5 | `yi` | 需查证 | 搜索 "Yi" 或 "01.AI" |
| InternLM3 | `internlm` | 需查证 | 搜索 "InternLM" |

> ⚠️ 添加新模型时，先去 https://lobehub.com/zh/icons 搜索对应图标，确认存在后再填写 icon 字段。

## 常见问题

### 下载 404
文件名大小写与 HuggingFace 不一致。用 API 验证精确文件名。

### 下载 401
仓库需认证或是 Gated Model，不能直接下载。换用公开仓库。

### ModelScope 上不到
部分 bartowski 社区量化的模型只存在于 HuggingFace，ModelScope 没有镜像。这不影响使用，Fallback 机制会自动跳过并尝试下一个源。

### 中国用户下载慢
正常情况下 HF-Mirror 是第一优先级，应该能走国内 CDN。如果 HF-Mirror 也慢，会自动切到 ModelScope。
