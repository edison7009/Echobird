# 本地 LLM 服务器方案

## 架构概述

WhichClaw 使用 llama.cpp 官方预编译的 `llama-server.exe` 来运行本地 GGUF 模型，**零编译、开箱即用**。

```
用户配置端口 (默认 11434)
  ├── /v1/*        → OpenAI 兼容 API（直接透传）
  └── /anthropic/* → Anthropic 兼容 API（自动格式转换）
          ↓
    llama-server.exe (内部端口 11534)
```

### 统一代理架构

- **llama-server.exe** 运行在内部端口（用户端口 + 100），仅监听 `127.0.0.1`
- **Node.js 统一代理** 运行在用户配置端口，对外暴露，根据路径自动路由：
  - `/v1/*` — 直接透传到 llama-server（OpenAI 原生支持）
  - `/anthropic/*` — 将 Anthropic 格式请求转换为 OpenAI 格式后转发，响应再转换回来

### 支持的 API 端点

| 端点 | 协议 | 流式 |
|------|------|------|
| `/v1/chat/completions` | OpenAI | ✅ |
| `/v1/models` | OpenAI | — |
| `/anthropic/v1/messages` | Anthropic | ✅ |

## 目录结构

```
electron/
├── ipc/
│   └── localModelHandlers.ts    # 核心入口：启动 llama-server + 统一代理
│
└── local/
    ├── bin/                      # 预编译二进制（.gitignore 忽略）
    │   ├── llama-b***-bin-win-cuda-*.*/   # llama-server.exe + DLL
    │   └── cudart-llama-bin-win-cuda-*.*/ # CUDA 运行时 DLL
    │
    └── tools/                    # Function Calling 工具定义
        ├── coding-tools.json     # 编程工具：read_file, write_to_file, list_directory, run_command
        └── web-tools.json        # 网页工具：search_web, read_url
```

## 二进制更新

从 [llama.cpp Releases](https://github.com/ggml-org/llama.cpp/releases) 下载：

1. `llama-b<版本>-bin-win-cuda-<CUDA版本>-x64.zip` — 主程序
2. `cudart-llama-bin-win-cuda-<CUDA版本>-x64.zip` — CUDA 运行时

解压到 `electron/local/bin/` 目录下即可。程序会自动扫描子目录查找 `llama-server.exe` 和 CUDA DLL。

### CUDA 版本选择

| 系统 CUDA 版本 | 推荐下载 |
|---------------|---------|
| 12.x | cuda-12.4 |
| 13.x | cuda-13.1 |

### 纯 CPU 版本

不需要 GPU 加速时，下载 `llama-b<版本>-bin-win-avx2-x64.zip` 即可，无需 CUDA。

## GPU 层数说明

| 设置 | 行为 |
|------|------|
| `-1` | 全部层加载到 GPU（需要足够显存） |
| `0` | 纯 CPU 推理 |
| `N` | 将 N 层 offload 到 GPU，其余用 CPU |

> 如果显存不足以放下所有层，llama-server 会在日志中提示，但仍会尝试加载。可根据日志中的提示调低 GPU 层数。
